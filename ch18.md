Rolling updates

Day1 Operations vs Day2 Operations

By default, when a scaled resource is updated:
- New pods are created 
- Old pods are terminated
- These things happen at the same time..
- Problematic if something goes wrong

---

With rolling updates, when a Deployment is updated, it happens progressively.

And so, the Deployment controls multiple ReplicaSets. Each ReplicaSet being a 
group of identical pods (with the same image, arguments parameters, etc.). 

Therefore, during the rolling update, we will have at least two ReplicaSets:
- The "new" set 
- At least one "old" set

---

Two parameteres determine the pace of the rollout: `maxUnavailable` and `maxSurge`.
At any given time:
- There will always be at least `replicas`-`maxUnavailable` pods available
- There will never be more than `replicas`+`maxSurge` pods in total
- There will therefore be up to `maxUnavailable`+`maxSurge` pods being updated

---

Checking the current rollout parameters:
```
kubectl get deploy -o json |
    jq ".items[] | {name:.metadata.name} + .spec.strategy.rollingUpdate"
```

---

We can do rolling updates with: `deployments`, `daemonsets`, `statefulness`. 
Editing one of these resources will automatically result in a rolling update.

These updates can be monitored with: `kubectl rollout`.

---

Recovering from a bad rollout: 
- Pushing an actual image for the rollout tag
- Cancel the deployment with:
```
kubectl rollout undo deploy worker
kubectl rollout status deploy worker

```
\* The undo command is flipping to the last one used. It's not going in a stack
of history. If you do it twice it will essentially reverse it .

---




