If we would like to remove a pod from the load balancer, the `kubectl delete pod` would re-create it immediately (by the replica set or the daemon set). The same thing would happen if we use the `app=rng` label.

---

The "mission" of a replica set is:
- "Make sure that there is the right number of pods matching this spec!"

The "mission" of a daemon set is:
- "Make sure that there is a pod matching this spec on each node!"

---

Since both the `rng` daemon set and the `rng` replica set use `app=rng`, the reason that they don't "find" each other's pods is:

- *Replica sets* have a more specific selector, visible with `kubectl describe` (it looks like `app=rng, pod-template-hash=abcd123`)
- *Daemon sets* also have a more specific selector, but it's invisible (it looks like `app=rng, controller-revision-hash=abcd123`)

As a result, each controller only "sees" the pods it manages.

---

If we want to remove a pod from the load balancer, we need to change the selector of the `rng` service.

If a selector specifies multiple labels, they are understood as a logical *AND*.

Therefore, what we could do is:

- Add the label `active=yes` to all our rng pods
- Update the selector for the rng service to also include `active=yes`
- Toggle traffic to a pod by manually adding/removing the `active` label

---


