Healthchecks:
- Are key to providing built-in *lifecycle automation*
- Are *probes* that apply to *containers* (not to pods)
- Kubernetes will *take action* on containers that fail healthchecks

---

Each container can have three (optional) probes:
- liveness = is this container dead or alive? (most important probe)
- readiness = is this container ready to serve traffic? (only needed if a 
service)
- startup = is this container still starting up? 

Probe *handlers* are available in HTTP, TCP, exec.

---

Liveness probe

If the liveness probe fails, the container is killed.

What happens next depends on the pod's `restartPolicy`:
- `Never`: the container is not restarted
- `OnFailure` or `Always`: the container is restarted

Liveness probes should be used to indicate failures that can't be recovered like:
- Deadlocks (causing all requests to time out)
- Internal corruption (causing all requests to error)

Anything where our incident response would be "just restart/reboot it".

\* Do not use liveness probes for problems that can't be fixed by a restart.
Otherwise we just restart our pods for no reason, creating useless load.

---

Readiness probe 

Indicates if the container is ready to server traffic.

If the readiness probe fails:
- The container is *not* killed
- If the pod is a member of a service, it is temporarily removed
- It is re-added as soon as the readiness probe passes again

To indicate failure due to an external cause
- Database is down or unreachable
- Mandatory auth or other backend service unavailable

To indicate temporary failure or unavailability
- Application can only service N parallel connections
- Runtime is busy doing garbage collection or initial data load

---

Startup probe

It can be used to indicate "container not ready *yet*":
- Process is still starting
- Loading external data, priming caches

Before, there was an `initialDelaySeconds` parameters.

---

Some of the benefits of using probes include:
- Rolling updates proceed when containers are *actually ready*
- Containers in a broken state get killed and restarted
- Unavailable backends get removed from load balancer rotation
- If a probe is not defined, it's as if there was an "always successful" probe

---

HTTP request
- Specify URL of the request (and optional headers)
- Any status code between 200 and 399 indicates success

TCP connection
- The probe succeeds if the TCP port is open

Arbitrary exec
- A command is executed in the container
- Exit status of zero indicates success

---

Probes are executed at intervals of `periodSeconds` (default: 10)
The timeout for a probe is set with `timeoutSeconds` (default: 1)

\* If a probe takes longer than that, it is considered as a FAIL.

A probe:
- Is considered successful after `successThreshold` successes (default: 1)
- Is considered successful after `failureThreshold` failures (default: 3)

---

Pod template for the `rng` web service of the DockerCoins app:

```
apiVersion: v1
kind: Pod
metadata:
  name: rng-with-liveness
spec:
  containers:
  - name: rng
    image: dockercoins/rng:v0.1
    livenessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 10
      periodSeconds: 1
```
Or a redis server:

```
apiVersion: v1
kind: Pod
metadata:
  name: redis-with-liveness
spec:
  containers:
  - name: redis
    image: redis
    livenessProbe:
      exec:
        command: ["redis-cli", "ping"]
```
---


